<!DOCTYPE html>
<html>
<head>
	<meta charset="utf-8" />
	<meta http-equiv="X-UA-Compatible" content="IE=edge"><title>Polynomial Regression - Brian Zhang&#39;s blog</title><meta name="viewport" content="width=device-width, initial-scale=1">
	<meta property="og:title" content="Polynomial Regression" />
<meta property="og:description" content="Introduction: side courses As a PhD student in the UK system, I was expecting a lot less coursework, with my first year diving straight into research. However, there are still a lot of gaps in my knowledge, so I hope to always be on the lookout for learning opportunities, including side classes.
At the moment, I’m hoping to follow along with these three courses and do some assignments from time to time:" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://brianzhang01.github.io/2017/11/polynomial-regression/" /><meta property="article:section" content="post" />
<meta property="article:published_time" content="2017-11-09T00:00:00+00:00" />
<meta property="article:modified_time" content="2017-11-09T00:00:00+00:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Polynomial Regression"/>
<meta name="twitter:description" content="Introduction: side courses As a PhD student in the UK system, I was expecting a lot less coursework, with my first year diving straight into research. However, there are still a lot of gaps in my knowledge, so I hope to always be on the lookout for learning opportunities, including side classes.
At the moment, I’m hoping to follow along with these three courses and do some assignments from time to time:"/>
<link href="https://fonts.googleapis.com/css?family=Ubuntu:300,400,300italic,400italic|Raleway:500,100,300" rel="stylesheet">

	<link rel="stylesheet" type="text/css" media="screen" href="https://brianzhang01.github.io/css/normalize.css" />
	<link rel="stylesheet" type="text/css" media="screen" href="https://brianzhang01.github.io/css/main.css" /><script src="https://cdn.jsdelivr.net/npm/feather-icons/dist/feather.min.js"></script><script src="https://brianzhang01.github.io/js/main.js"></script>
</head>

<body>
	<div class="container wrapper post">
		<div class="header">
	<h1 class="site-title">Brian Zhang&#39;s blog</h1>
	<div class="site-description"><h2>Statistics and other topics</h2><nav class="nav social">
			<ul class="flat"><a href="https://github.com/brianzhang01" title="Github"><i data-feather="github"></i></a><a href="https://twitter.com/brianczhang" title="Twitter"><i data-feather="twitter"></i></a></ul>
		</nav>
	</div>

	<nav class="nav">
		<ul class="flat">
			
			<li>
				<a href="/">Home</a>
			</li>
			
			<li>
				<a href="/post">All posts</a>
			</li>
			
			<li>
				<a href="/about">About</a>
			</li>
			
			<li>
				<a href="/tags">Tags</a>
			</li>
			
		</ul>
	</nav>
</div>


		<div class="post-header">
			<h1 class="title">Polynomial Regression</h1>
			<div class="meta">Posted Nov 9, 2017 &middot; 8 min read</div>
		</div>

		<div class="markdown">
			


<div id="introduction-side-courses" class="section level2">
<h2>Introduction: side courses</h2>
<p>As a PhD student in the UK system, I was expecting a lot less coursework, with my first year diving straight into research. However, there are still a lot of gaps in my knowledge, so I hope to always be on the lookout for learning opportunities, including side classes.</p>
<p>At the moment, I’m hoping to follow along with these three courses and do some assignments from time to time:</p>
<ul>
<li>With a few other Oxford students, I’m keeping up with the <a href="http://www.gatsby.ucl.ac.uk/teaching/courses/ml1/">machine learning course</a> at UCL’s Gatsby Computational Neuroscience Unit, taught by Maneesh Sahani. I’m hoping to refresh the statistical foundations of machine learning, and particularly learn approximate inference. A previous year’s course materials are <a href="http://www.gatsby.ucl.ac.uk/teaching/courses/ml1-2016.html">here</a>.</li>
<li>My PhD is in statistical genetics, but my background is primarily in machine learning. So I’ve been working through Barbara Englehardt’s Spring 2013 Duke course, <a href="https://www.cs.princeton.edu/~bee/courses/cbb540.html">STA613/CBB540: Statistical methods in computational biology</a>. The first assignment was also a really smooth introduction to learning R.</li>
<li>While at Harvard, I took two semesters of machine learning with Ryan Adams. The second semester was a graduate class, Fall 2013 of <a href="https://www.seas.harvard.edu/courses/cs281/">CS 281: Advanced Machine Learning</a>, and was a lot more difficult. I didn’t get around to finishing all the assignments, so have intended to go back and solve some of the problems, including Assignment 5 on Gaussian Processes and Bayesian Nonparametrics which I skipped, since we could drop one lowest grade.</li>
</ul>
<p>All materials from these classes are online at the above links, so I would recommend checking them out!</p>
</div>
<div id="data-from-a-cs-281-assignment" class="section level2">
<h2>Data from a CS 281 assignment</h2>
<p>In this post, I’ll work through an example of polynomial regression, while illustrating some R features like <code>ggplot</code> and R Markdown documents. The data that I’ll use comes from the CS 281 course linked to above, in problem 4 of <a href="https://www.seas.harvard.edu/courses/cs281/files/assignment-2.pdf">assignment 2</a>:</p>
<pre><code>Here are some simple data to regress:

x = [-1.87 -1.76 -1.67 -1.22 -0.07 0.11 0.67 1.60 2.22 2.51]&#39;
y = [0.06 1.67 0.54 -1.45 -0.18 -0.67 0.92 2.95 5.13 5.18]&#39;

Construct a Bayesian linear regression model using a basis of your choosing (e.g., polynomial, sinusoids, radial basis functions). Choose priors that seem sensible for the regression weights and the Gaussian noise.</code></pre>
<p>While taking the course, I didn’t manage to fully solve the problem. However, for now, I’ll be using the data points as a demonstration not of Bayesian regression, but the simpler case of least-squares regression without a prior.</p>
<p>To start, we can create variables for our data points and plot them using <code>ggplot</code>.</p>
<pre class="r"><code>library(ggplot2)
center_title = theme(plot.title = element_text(hjust = 0.5))
x = c(-1.87, -1.76, -1.67, -1.22, -0.07, 0.11, 0.67, 1.60, 2.22, 2.51)
y = c(0.06, 1.67, 0.54, -1.45, -0.18, -0.67, 0.92, 2.95, 5.13, 5.18)
ggplot(data.frame(x = x, y = y), aes(x=x, y=y)) + geom_point() +
  labs(title = &quot;Points to regress&quot;) + center_title</code></pre>
<p><img src="/post/2017-11-09-polynomial-regression_files/figure-html/data-1.png" width="672" /></p>
</div>
<div id="a-simple-linear-regression" class="section level2">
<h2>A simple linear regression</h2>
<p>Linear least-squares regression seeks to learn the parameters <span class="math inline">\(\beta_0, \beta_1\)</span> that minimize <span class="math display">\[
\sum_i (y_i-(\beta_0 + \beta_1 x_i))^2
\]</span> If we introduce matrices <span class="math display">\[
\mathbf{y} = \begin{pmatrix} y_1\\ y_2\\ \vdots \\ y_n \end{pmatrix},
\mathbf{X} = \begin{pmatrix}
1 &amp; x_1\\
1 &amp; x_2\\
\vdots &amp; \vdots\\
1 &amp; x_n\\
\end{pmatrix},
\boldsymbol{\beta} = \begin{pmatrix} \beta_0 \\ \beta_1 \end{pmatrix}
\]</span> The loss function can be written as <span class="math display">\[
l(\boldsymbol{\beta}; \mathbf{X}, \mathbf{y}) = (\mathbf{y}-\mathbf{X}\boldsymbol{\beta})^\intercal (\mathbf{y}-\mathbf{X}\boldsymbol{\beta})
\]</span> Without going through all the algebra, the minimum of this function with respect to <span class="math inline">\(\boldsymbol{\beta}\)</span> can be obtained by setting the derivative to zero. After using some matrix identities, the solution is <span class="math display">\[
\mathbf{\hat{\boldsymbol{\beta}}} = (\mathbf{X}^\intercal\mathbf{X})^{-1} \mathbf{X}^\intercal \mathbf{y}
\]</span> In R, the function <code>lm</code> can solve linear regression for us, but instead we can also code up the solution manually. First, we set up our matrices <span class="math inline">\(\mathbf{X}\)</span> and <span class="math inline">\(\mathbf{y}\)</span>:</p>
<pre class="r"><code>x_matrix = cbind(rep(1, 10), x)
x_matrix</code></pre>
<pre><code>##             x
##  [1,] 1 -1.87
##  [2,] 1 -1.76
##  [3,] 1 -1.67
##  [4,] 1 -1.22
##  [5,] 1 -0.07
##  [6,] 1  0.11
##  [7,] 1  0.67
##  [8,] 1  1.60
##  [9,] 1  2.22
## [10,] 1  2.51</code></pre>
<pre class="r"><code>y_matrix = as.matrix(y)
y_matrix</code></pre>
<pre><code>##        [,1]
##  [1,]  0.06
##  [2,]  1.67
##  [3,]  0.54
##  [4,] -1.45
##  [5,] -0.18
##  [6,] -0.67
##  [7,]  0.92
##  [8,]  2.95
##  [9,]  5.13
## [10,]  5.18</code></pre>
<p>Next we write the matrix algebra solution for computing <span class="math inline">\(\mathbf{\hat{\boldsymbol{\beta}}}\)</span>. If you’re new to R, the notation may be unfamiliar to you, and you may want to consult <a href="https://www.statmethods.net/advstats/matrix.html">this cheatsheet</a>.<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a></p>
<pre class="r"><code>beta = solve(t(x_matrix) %*% x_matrix) %*% t(x_matrix) %*% y_matrix
beta</code></pre>
<pre><code>##       [,1]
##   1.359589
## x 1.065601</code></pre>
<p>Finally, we plot the result.</p>
<pre class="r"><code>ggplot(data.frame(x = x, y = y), aes(x=x, y=y)) + geom_point() +
  labs(title = &quot;Linear regression&quot;) + center_title +
  geom_abline(slope = beta[2], intercept = beta[1])</code></pre>
<p><img src="/post/2017-11-09-polynomial-regression_files/figure-html/linear-plot-1.png" width="672" /></p>
</div>
<div id="generalizing-to-polynomial-regression" class="section level2">
<h2>Generalizing to polynomial regression</h2>
<p>What if instead of fitting a line, we wanted to fit a quadratic to our data? Our quadratic would then have three parameters, and we would want to minimize: <span class="math display">\[
\sum_i (y_i-(\beta_0 + \beta_1 x_i + \beta_2 x_i^2))^2
\]</span> This loss function <span class="math inline">\(l(\boldsymbol{\beta}; \mathbf{X}, \mathbf{y})\)</span> can be put in the exact same form as before, as long as we instead define our matrices as:<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a> <span class="math display">\[
\mathbf{y} = \begin{pmatrix} y_1\\ y_2\\ \vdots \\ y_n \end{pmatrix},
\mathbf{X} = \begin{pmatrix}
1 &amp; x_1 &amp; x_1^2\\
1 &amp; x_2 &amp; x_2^2\\
\vdots &amp; \vdots &amp; \vdots \\
1 &amp; x_n &amp; x_n^2\\
\end{pmatrix},
\boldsymbol{\beta} = \begin{pmatrix} \beta_0 \\ \beta_1 \\ \beta_2 \end{pmatrix}
\]</span> We then get: <span class="math display">\[\begin{gather*}
l(\boldsymbol{\beta}; \mathbf{X}, \mathbf{y}) = (\mathbf{y}-\mathbf{X}\boldsymbol{\beta})^\intercal (\mathbf{y}-\mathbf{X}\boldsymbol{\beta})\\
\mathbf{\hat{\boldsymbol{\beta}}} = (\mathbf{X}^\intercal\mathbf{X})^{-1} \mathbf{X}^\intercal \mathbf{y}
\end{gather*}\]</span></p>
<p>It should be clear that this pattern continues on, at least until the degree <span class="math inline">\(d\)</span> of the polynomial equals <span class="math inline">\(n - 1\)</span>. At that point, we are able to hit all our <span class="math inline">\(n\)</span> points exactly (assuming the <span class="math inline">\(x\)</span>-coordinates are all different), since the polynomial has <span class="math inline">\(n\)</span> free parameters. If we then take <span class="math inline">\(d \geq n\)</span>, there are multiple polynomial solutions, and we should no longer trust our expression to give the single right solution.<a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a></p>
<p>Using the above result, we can perform polynomial regression on our <span class="math inline">\(n = 10\)</span> data points with <span class="math inline">\(d = 0\)</span> to <span class="math inline">\(9\)</span>. Once we have the <span class="math inline">\(\boldsymbol{\beta}\)</span> coefficients, we sweep over an <span class="math inline">\(x\)</span>-range from -2.5 to 3 and generate the corresponding <span class="math inline">\(y\)</span> value for the polynomial. We keep the resulting polynomial data in a data frame.</p>
<pre class="r"><code>x_matrix = NULL
y_matrix = as.matrix(y)
x_curve = seq(-2.5, 3, 0.05)
y_data = NULL
for(i in 1:10) {
  x_matrix = cbind(x_matrix, x^(i-1))
  beta = solve(t(x_matrix) %*% x_matrix) %*% t(x_matrix) %*% y_matrix
  y_curve = rep(0, length(x_curve))
  for(j in 1:i) {
    y_curve = y_curve + beta[j]*x_curve^(j-1)
  }
  y_data = rbind(y_data, data.frame(x=x_curve, y=y_curve, d=(i-1)))
}</code></pre>
<p>Let’s take a look at our final matrix for <span class="math inline">\(\mathbf{X}\)</span> for <span class="math inline">\(d=9\)</span>.</p>
<pre class="r"><code>round(x_matrix, digits=2)</code></pre>
<pre><code>##       [,1]  [,2] [,3]  [,4]  [,5]   [,6]   [,7]   [,8]    [,9]   [,10]
##  [1,]    1 -1.87 3.50 -6.54 12.23 -22.87  42.76 -79.96  149.53 -279.62
##  [2,]    1 -1.76 3.10 -5.45  9.60 -16.89  29.72 -52.31   92.07 -162.04
##  [3,]    1 -1.67 2.79 -4.66  7.78 -12.99  21.69 -36.23   60.50 -101.03
##  [4,]    1 -1.22 1.49 -1.82  2.22  -2.70   3.30  -4.02    4.91   -5.99
##  [5,]    1 -0.07 0.00  0.00  0.00   0.00   0.00   0.00    0.00    0.00
##  [6,]    1  0.11 0.01  0.00  0.00   0.00   0.00   0.00    0.00    0.00
##  [7,]    1  0.67 0.45  0.30  0.20   0.14   0.09   0.06    0.04    0.03
##  [8,]    1  1.60 2.56  4.10  6.55  10.49  16.78  26.84   42.95   68.72
##  [9,]    1  2.22 4.93 10.94 24.29  53.92 119.71 265.75  589.96 1309.71
## [10,]    1  2.51 6.30 15.81 39.69  99.63 250.06 627.65 1575.40 3954.24</code></pre>
<p>Here is what some of our output data frame looks like:</p>
<pre class="r"><code>head(y_data[y_data$d == 2,])</code></pre>
<pre><code>##         x        y d
## 223 -2.50 1.970514 2
## 224 -2.45 1.843023 2
## 225 -2.40 1.718879 2
## 226 -2.35 1.598079 2
## 227 -2.30 1.480625 2
## 228 -2.25 1.366517 2</code></pre>
<p>And now, we plot the results. Plotting all the polynomials at once makes the graph too cluttered, so instead we only show two plots that help illustrate different behavior.</p>
<pre class="r"><code>ggplot() + 
  geom_point(data=data.frame(x = x, y = y), aes(x=x, y=y)) +
  geom_line(data=y_data[y_data$d &lt; 4,],
            aes(x=x, y=y, color=factor(d), group=factor(d))) +
  labs(title = &quot;Polynomial least-squares regression&quot;, color=&quot;dimension&quot;) +
  center_title</code></pre>
<p><img src="/post/2017-11-09-polynomial-regression_files/figure-html/poly-plot-1.png" width="672" /></p>
<pre class="r"><code>ggplot() + 
  geom_point(data=data.frame(x = x, y = y), aes(x=x, y=y)) +
  geom_line(data=y_data[y_data$d %% 2 == 1,],
            aes(x=x, y=y, color=factor(d), group=factor(d))) +
  labs(title = &quot;Polynomial least-squares regression&quot;, color=&quot;dimension&quot;) +
  center_title +
  coord_cartesian(ylim = c(-3, 8))</code></pre>
<p><img src="/post/2017-11-09-polynomial-regression_files/figure-html/poly-plot-2.png" width="672" /></p>
</div>
<div id="summary" class="section level2">
<h2>Summary</h2>
<p>Hopefully, I’ve impressed upon you that:</p>
<ul>
<li>Polynomial least-squares regression is simply multiple linear regression with an expanded basis.</li>
<li><code>ggplot</code>’s are easy to make in R and look nice!</li>
</ul>
<p>In a future post, I may circle back to this data and solve more of the original CS 281 assignment involving Bayesian regression.</p>
<p>Thanks to:</p>
<ul>
<li>Ryan Adams for teaching CS 281 and making the course materials freely available.</li>
<li>William Chen for some <code>ggplot</code> starter code that helped me when I was first learning.</li>
<li>Constantin Ahlmann-Eltze for calling my attention to the differences between setting <code>ylim</code> directly versus inside <code>cartesian_coord</code> when using <code>ggplot</code>.</li>
</ul>
<p><strong><em>This blog post was generated from an R Markdown file using the <code>knitr</code> and <code>blogdown</code> packages. The original source can be downloaded <a href="https://github.com/brianzhang01/brianzhang01.github.io/blob/master/post/2017-11-09-polynomial-regression.Rmd">from GitHub</a>.</em></strong></p>
<p><strong><em>UPDATE 2017-11-21: added a reference to the Vandermonde matrix and made some other small fixes.</em></strong> <strong><em>UPDATE 2020-02-04: added a link showing that <span class="math inline">\(\text{rank}(\mathbf{A}^\intercal\mathbf{A}) = \text{rank}(\mathbf{A})\)</span>.</em></strong></p>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>Another note for new R users: you can pull up the documentation for a command like <code>solve</code> by executing <code>?solve</code> in the console.<a href="#fnref1" class="footnote-back">↩</a></p></li>
<li id="fn2"><p>Thanks to Shoucheng Zhang for pointing out that the general <span class="math inline">\(\mathbf{X}\)</span> matrix is known as the <a href="https://en.wikipedia.org/wiki/Vandermonde_matrix">Vandermonde matrix</a>.<a href="#fnref2" class="footnote-back">↩</a></p></li>
<li id="fn3"><p>Interestingly, I tried this out on our data using <span class="math inline">\(d=n=10\)</span>. The math expression is <span class="math display">\[
\mathbf{\hat{\boldsymbol{\beta}}} = (\mathbf{X}^\intercal\mathbf{X})^{-1} \mathbf{X}^\intercal \mathbf{y},
\]</span> and when I get to the inversion step, I receive an error message:</p>
<pre><code>Error in solve.default(t(x_matrix) %*% x_matrix) : 
system is computationally singular: reciprocal condition number = 1.10547e-20</code></pre>
<p>This suggests that for <span class="math inline">\(d \geq n\)</span>, the matrix <span class="math inline">\((\mathbf{X}^\intercal\mathbf{X})\)</span> is no longer full-rank. Indeed, looking online, I <a href="https://stats.stackexchange.com/questions/49889/why-does-the-rank-of-the-design-matrix-x-equal-the-rank-of-xx">seem to find</a> that for any real matrix <span class="math inline">\(\mathbf{A}\)</span>, <span class="math inline">\(\text{rank}(\mathbf{A}^\intercal\mathbf{A}) = \text{rank}(\mathbf{A}) = \text{rank}(\mathbf{A}^\intercal)\)</span>. In this case, where <span class="math inline">\(\mathbf{X}\)</span> is <span class="math inline">\(n\)</span> by <span class="math inline">\((d+1)\)</span>, we have <span class="math inline">\(\text{rank}(\mathbf{X}^\intercal\mathbf{X}) = \text{rank}(\mathbf{X}^\intercal) \leq n\)</span>, the number of columns of <span class="math inline">\(\mathbf{X}^\intercal\)</span>, but <span class="math inline">\(\mathbf{X}^\intercal\mathbf{X}\)</span> is a <span class="math inline">\((d+1)\)</span> by <span class="math inline">\((d+1)\)</span> matrix, so it cannot be full-rank.</p>
<p>For a similar presentation in Python, see <a href="http://jakevdp.github.io/blog/2015/07/06/model-complexity-myth/#The-Mathematics-of-Underdetermined-Models">this blog post</a> by Jake VanderPlas.<a href="#fnref3" class="footnote-back">↩</a></p></li>
</ol>
</div>

		</div><div id="disqus_thread"></div>
<script type="text/javascript">
	(function () {
		
		
		if (window.location.hostname == "localhost")
			return;

		var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
		var disqus_shortname = 'brianzhang01-stats';
		dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
		(document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
	})();
</script>
<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by
		Disqus.</a></noscript>
<a href="http://disqus.com/" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
</div>
	<div class="footer wrapper">
	<nav class="nav">
		<div> Built with <a href="https://gohugo.io">Hugo</a>, <a href="https://github.com/rstudio/blogdown">blogdown</a>, and <a href="https://github.com/vividvilla/ezhil">Ezhil theme</a></div>
	</nav>
</div>

<script src="//yihui.name/js/math-code.js"></script>
<script async
src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'UA-113303717-1', 'auto');
	
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>
<script>feather.replace()</script>
</body>
</html>
