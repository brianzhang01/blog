<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Brian Zhang&#39;s blog</title>
    <link>https://www.brianczhang.com/blog/post/</link>
    <description>Recent content in Posts on Brian Zhang&#39;s blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 20 Apr 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://www.brianczhang.com/blog/post/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>On Dedekind</title>
      <link>https://www.brianczhang.com/blog/2025/04/on-dededkind/</link>
      <pubDate>Sun, 20 Apr 2025 00:00:00 +0000</pubDate>
      
      <guid>https://www.brianczhang.com/blog/2025/04/on-dededkind/</guid>
      <description>An extended quote from John Stilwell’s Elements of Algebra, p. 35-36:
“[Gauss’s] Disquisitiones became the bible of the next generation of number theorists, particularly Dirichlet, who kept a copy of it on his desk at all times. Dirichlet’s lectures became a classic in their turn when edited by Dedekind as the book Vorlesungen über Zahlentheorie. The first edition appeared in 1863 (four years after Dirichlet’s death) and the book gradually changed character as Dedekind added appendices in subsequent editions.</description>
    </item>
    
    <item>
      <title>Analyzing &#34;Objection Funk&#34;</title>
      <link>https://www.brianczhang.com/blog/2025/04/objection-funk/</link>
      <pubDate>Sat, 12 Apr 2025 00:00:00 +0000</pubDate>
      
      <guid>https://www.brianczhang.com/blog/2025/04/objection-funk/</guid>
      <description>I wrote this on 2025-01-29.
My friend Alex Irpan has inspired me that it&amp;rsquo;s possible to mix serious and meme content on the same blog. So I&amp;rsquo;m returning to blogging with a serious take on a stupid topic.
 &amp;ldquo;Objection Funk&amp;rdquo; by iteachvader is a trolling, fan piece of Internet content. At the same time, it has great relistening / rewatching value. The background funk track definitely plays a role in its appeal.</description>
    </item>
    
    <item>
      <title>LeetCode Interview Prep</title>
      <link>https://www.brianczhang.com/blog/2025/04/leetcode/</link>
      <pubDate>Sat, 12 Apr 2025 00:00:00 +0000</pubDate>
      
      <guid>https://www.brianczhang.com/blog/2025/04/leetcode/</guid>
      <description>This is a backfill note from 2019-09-09. In 2019, I mentored a college student who was applying for computer science internships. I helped source LeetCode problems that I thought were interesting; here is the list I came up with.
(It looks like the numbering has changed since I compiled this list.)
LeetCode interview prep LinkedList  Add Two Numbers
  Merge Two Sorted Lists
  Remove Duplicates from Sorted List II</description>
    </item>
    
    <item>
      <title>Motivating Problems in Math</title>
      <link>https://www.brianczhang.com/blog/2025/04/math-motivation/</link>
      <pubDate>Sat, 12 Apr 2025 00:00:00 +0000</pubDate>
      
      <guid>https://www.brianczhang.com/blog/2025/04/math-motivation/</guid>
      <description>This is a backfill note from 2019-07-28. The idea is, what are mathematical achievements that are useful for motivating the study of mathematics?
Motivating problems in math Solved and important!  Infinite number of primes Sqrt(2) is irrational Fundamental theorem of algebra (1806) Unsolvability of the quintic (1824) Three construction problems of antiquity (1837, 1837, 1882) e and pi are transcendental (1873, 1882) Public-key cryptography (Diffie-Hellman-Merkle, RSA, elliptic curves) (1973-1976) Fermat’s Last Theorem / modularity theorem (1994, 2001) Poincare Conjecture (2003) Classification of finite simple groups (2004)  Axiomatics  Parallel Postulate (1829-1868) Gödel’s Incompleteness Theorems (1931) Continuum Hypothesis (1940, 1963)  Solved and unimportant, but with an interesting anecdote  Hilbert’s 3rd Problem (Dehn Invariants, 1883-1900, first to be solved) Four color theorem (1976) Kepler conjecture and honeycomb conjecture (1998, 1999, Thomas Hales)  Unsolved  Goldbach Conjecture Twin prime conjecture Perfect numbers / Mersenne primes Collatz Conjecture Whether Euler-Mascheroni constant is rational / algebraic or not Riemann Hypothesis Ramsey numbers (R(5, 5), R(6, 6)) ABC Conjecture Birch and Swinnerton-Dyer Conjecture  </description>
    </item>
    
    <item>
      <title>A Regularization Proof</title>
      <link>https://www.brianczhang.com/blog/2022/09/a-regularization-proof/</link>
      <pubDate>Mon, 19 Sep 2022 00:00:00 +0000</pubDate>
      
      <guid>https://www.brianczhang.com/blog/2022/09/a-regularization-proof/</guid>
      <description>Say we have a loss function \(l(w)\). With no regularization, we might obtain the minimum at \(w = w_0\). Now consider the setting with regularization: \[ f_\lambda(w) = l(w) + \lambda R(w), \] where \(R(w) \geq 0\) is some regularization function and \(\lambda \geq 0\). What can we say if we consider the minimizing inputs \(w_1\) for \(f_{\lambda_1}(w)\) and \(w_2\) for \(f_{\lambda_2}(w)\), with \(0 \leq \lambda_1 &amp;lt; \lambda_2\)? \[ w_1 = argmin_w \left[ l(w) + \lambda_1 R(w) \right],\\ w_2 = argmin_w \left[ l(w) + \lambda_2 R(w) \right].</description>
    </item>
    
    <item>
      <title>On NumPy Multithreading</title>
      <link>https://www.brianczhang.com/blog/2020/02/on-numpy-multithreading/</link>
      <pubDate>Tue, 04 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>https://www.brianczhang.com/blog/2020/02/on-numpy-multithreading/</guid>
      <description>Two notes. First, numpy supports multithreading, and this can give you a speed boost in multicore environments! On Linux, I used top to verify that my numpy was indeed using multithreading, which it was. Second, multithreading can hurt performance when you&amp;rsquo;re running multiple Python / numpy processes at once. I was running into this issue, and got significant boost by limiting the number of numpy threads per process, in my case using import mkl; mkl.</description>
    </item>
    
    <item>
      <title>Fast Hierarchical Clustering Using fastcluster</title>
      <link>https://www.brianczhang.com/blog/2019/10/fast-hierarchical-clustering-using-fastcluster/</link>
      <pubDate>Fri, 25 Oct 2019 00:00:00 +0000</pubDate>
      
      <guid>https://www.brianczhang.com/blog/2019/10/fast-hierarchical-clustering-using-fastcluster/</guid>
      <description>Do you use hierarchical clustering packages like R’s hclust or Python’s scipy.cluster.hierarchy.linkage in your workflow? If so, you’re using an \(O(N^3)\) algorithm1 and should switch to the fastcluster package, which provides \(O(N^2)\) routines for the most commonly used types of clustering.
fastcluster is implemented in C++, with interfaces for C++, R, and Python. In particular, the Python interface mirrors scipy.cluster.hierarchy.linkage, and the R interface mirrors stats::hclust and flashClust::flashClust, so switching over is a no-brainer.</description>
    </item>
    
    <item>
      <title>Software Engineering Tools Across 4 Languages</title>
      <link>https://www.brianczhang.com/blog/2019/09/software-engineering-tools-across-4-languages/</link>
      <pubDate>Sat, 28 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>https://www.brianczhang.com/blog/2019/09/software-engineering-tools-across-4-languages/</guid>
      <description>It&amp;rsquo;s been two years since I started blogging, as well as two years since I started my PhD in the Oxford Statistics department. While it&amp;rsquo;s been several months since my last post, I hope to get back into sharing some shorter posts and ideas going forward.
To get myself writing again, I thought I&amp;rsquo;d broaden the scope of my posts beyond statistics and machine learning. In particular, over the past two years, I&amp;rsquo;ve found myself getting more interested in abstract math as well as software engineering.</description>
    </item>
    
    <item>
      <title>Subtle Observations on Range Queries</title>
      <link>https://www.brianczhang.com/blog/2019/01/subtle-observations-on-range-queries/</link>
      <pubDate>Tue, 22 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>https://www.brianczhang.com/blog/2019/01/subtle-observations-on-range-queries/</guid>
      <description>For my current research, I’ve had to read Kelleher et al.’s excellent msprime paper (2016) for simulating genetic sequences under the coalescent with recombination. A small trick that is used in their algorithm is the data structure of a Fenwick tree or binary indexed tree. Since I also have a side interest in competitive programming (mainly through USACO and Project Euler), I took a bit more time to learn this data structure.</description>
    </item>
    
    <item>
      <title>Missing Heritability and Microaggressions</title>
      <link>https://www.brianczhang.com/blog/2018/10/missing-heritability-and-microaggressions/</link>
      <pubDate>Wed, 24 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>https://www.brianczhang.com/blog/2018/10/missing-heritability-and-microaggressions/</guid>
      <description>Missing heritability is like microaggressions: many seemingly insignificant effects can add up.
Two weeks ago, as I was taking a journey back to London Heathrow / Oxford, I came across a small connection in a journal article and a podcast. It was a nice moment of seeing two ideas click together.
The podcast, which came second, was an episode of Nomad, a British podcast discussing Christian faith outside the institutional church.</description>
    </item>
    
    <item>
      <title>Random Graphs and Giant Components</title>
      <link>https://www.brianczhang.com/blog/2018/07/random-graphs-and-giant-components/</link>
      <pubDate>Tue, 10 Jul 2018 00:00:00 +0000</pubDate>
      
      <guid>https://www.brianczhang.com/blog/2018/07/random-graphs-and-giant-components/</guid>
      <description>This post will introduce some of the ideas behind random graphs, a very exciting area of current probability research. As has been a theme in my posts so far, I try to emphasize a reproducible, computational example. In this case, we’ll be looking at the “giant component” and how that arises in random graphs.
There’s a lot more than this example that I find exciting, so I’ve deferred a longer discussion on random graphs to the end of this post, with a lot of references for the interested reader.</description>
    </item>
    
    <item>
      <title>Distributions with SymPy</title>
      <link>https://www.brianczhang.com/blog/2018/04/distributions-with-sympy/</link>
      <pubDate>Wed, 04 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>https://www.brianczhang.com/blog/2018/04/distributions-with-sympy/</guid>
      <description>Any good statistics student will need to do some integrals in her / his life. While I generally feel comfortable with simple integrals, I thought it might be worth setting up a workflow to help automate this process!
Previously, especially coming from a physics background, I’ve worked a lot with Mathematica, an advanced version of the software available online as WolframAlpha. Mathematica is extremely powerful, but it’s not open-source and comes with a hefty license, so I decided to research alternatives.</description>
    </item>
    
    <item>
      <title>Clustering with K-Means and EM</title>
      <link>https://www.brianczhang.com/blog/2018/01/clustering-with-k-means-and-em/</link>
      <pubDate>Tue, 30 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>https://www.brianczhang.com/blog/2018/01/clustering-with-k-means-and-em/</guid>
      <description>Introduction K-means and EM for Gaussian mixtures are two clustering algorithms commonly covered in machine learning courses. In this post, I’ll go through my implementations on some sample data.
I won’t be going through much theory, as that can be easily found elsewhere. Instead I’ve focused on highlighting the following:
 Pretty visualizations in ggplot, with the helper packages deldir, ellipse, and knitr for animations.
 Structural similarities in the algorithms, by splitting up K-means into an E and M step.</description>
    </item>
    
    <item>
      <title>Statistics / ML Books</title>
      <link>https://www.brianczhang.com/blog/2017/11/statistics-ml-books/</link>
      <pubDate>Sat, 25 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>https://www.brianczhang.com/blog/2017/11/statistics-ml-books/</guid>
      <description>At the start of the last post, I talked briefly about courses I’ve been working through. Here are some follow-up thoughts on good books!^[This post is strategically placed so I can cite some of these textbooks in later posts.]
This post will focus on textbooks with a machine learning focus. I’ve read less of the classic statistics textbooks, as I hadn’t specialized much in statistics until my PhD. However, these are a few texts that are on my radar to consult:</description>
    </item>
    
    <item>
      <title>Polynomial Regression</title>
      <link>https://www.brianczhang.com/blog/2017/11/polynomial-regression/</link>
      <pubDate>Thu, 09 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>https://www.brianczhang.com/blog/2017/11/polynomial-regression/</guid>
      <description>Introduction: side courses As a PhD student in the UK system, I was expecting a lot less coursework, with my first year diving straight into research. However, there are still a lot of gaps in my knowledge, so I hope to always be on the lookout for learning opportunities, including side classes.
At the moment, I’m hoping to follow along with these three courses and do some assignments from time to time:</description>
    </item>
    
    <item>
      <title>Blogging Aims</title>
      <link>https://www.brianczhang.com/blog/2017/11/blogging-aims/</link>
      <pubDate>Sat, 04 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>https://www.brianczhang.com/blog/2017/11/blogging-aims/</guid>
      <description>Hi there, and thanks for stopping by! In this post, I briefly introduce my current ideas for this blog and say a bit about myself.
As of September, I&amp;rsquo;ve been a first-year PhD student at Oxford&amp;rsquo;s Statistics department. I received my bachelor&amp;rsquo;s in Physics from Harvard in 2015, and after working for two years am excited to be back in an academic setting. Part of this transition means more freedom and a lot more self-structured learning time.</description>
    </item>
    
  </channel>
</rss>
