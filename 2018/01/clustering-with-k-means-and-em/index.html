<!DOCTYPE html>
<html>
<head>
	<meta charset="utf-8" />
	<meta http-equiv="X-UA-Compatible" content="IE=edge"><title>Clustering with K-Means and EM - Brian Zhang&#39;s blog</title><meta name="viewport" content="width=device-width, initial-scale=1">
	<meta property="og:title" content="Clustering with K-Means and EM" />
<meta property="og:description" content="Introduction K-means and EM for Gaussian mixtures are two clustering algorithms commonly covered in machine learning courses. In this post, I’ll go through my implementations on some sample data.
I won’t be going through much theory, as that can be easily found elsewhere. Instead I’ve focused on highlighting the following:
 Pretty visualizations in ggplot, with the helper packages deldir, ellipse, and knitr for animations.
 Structural similarities in the algorithms, by splitting up K-means into an E and M step." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://brianzhang01.github.io/2018/01/clustering-with-k-means-and-em/" /><meta property="article:section" content="post" />
<meta property="article:published_time" content="2018-01-30T00:00:00+00:00" />
<meta property="article:modified_time" content="2018-01-30T00:00:00+00:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Clustering with K-Means and EM"/>
<meta name="twitter:description" content="Introduction K-means and EM for Gaussian mixtures are two clustering algorithms commonly covered in machine learning courses. In this post, I’ll go through my implementations on some sample data.
I won’t be going through much theory, as that can be easily found elsewhere. Instead I’ve focused on highlighting the following:
 Pretty visualizations in ggplot, with the helper packages deldir, ellipse, and knitr for animations.
 Structural similarities in the algorithms, by splitting up K-means into an E and M step."/>
<link href="https://fonts.googleapis.com/css?family=Ubuntu:300,400,300italic,400italic|Raleway:500,100,300" rel="stylesheet">

	<link rel="stylesheet" type="text/css" media="screen" href="https://brianzhang01.github.io/css/normalize.css" />
	<link rel="stylesheet" type="text/css" media="screen" href="https://brianzhang01.github.io/css/main.css" /><script src="https://cdn.jsdelivr.net/npm/feather-icons/dist/feather.min.js"></script><script src="https://brianzhang01.github.io/js/main.js"></script>
</head>

<body>
	<div class="container wrapper post">
		<div class="header">
	<h1 class="site-title">Brian Zhang&#39;s blog</h1>
	<div class="site-description"><h2>Statistics and other topics</h2><nav class="nav social">
			<ul class="flat"><a href="https://github.com/brianzhang01" title="Github"><i data-feather="github"></i></a><a href="https://twitter.com/brianczhang" title="Twitter"><i data-feather="twitter"></i></a></ul>
		</nav>
	</div>

	<nav class="nav">
		<ul class="flat">
			
			<li>
				<a href="/">Home</a>
			</li>
			
			<li>
				<a href="/post">All posts</a>
			</li>
			
			<li>
				<a href="/about">About</a>
			</li>
			
			<li>
				<a href="/tags">Tags</a>
			</li>
			
		</ul>
	</nav>
</div>


		<div class="post-header">
			<h1 class="title">Clustering with K-Means and EM</h1>
			<div class="meta">Posted Jan 30, 2018 &middot; 10 min read</div>
		</div>

		<div class="markdown">
			


<div id="introduction" class="section level2">
<h2>Introduction</h2>
<p>K-means and EM for Gaussian mixtures are two clustering algorithms commonly covered in machine learning courses. In this post, I’ll go through my implementations on some sample data.</p>
<p>I won’t be going through much theory, as that can be easily found elsewhere. Instead I’ve focused on highlighting the following:</p>
<ul>
<li><p>Pretty visualizations in <code>ggplot</code>, with the helper packages <code>deldir</code>, <code>ellipse</code>, and <code>knitr</code> for animations.</p></li>
<li><p>Structural similarities in the algorithms, by splitting up K-means into an E and M step.</p></li>
<li><p>Animations showing that EM reduces to the K-means algorithm in a particular limit.</p></li>
</ul>
<p>This last point is covered in Section 9.3.2 of Bishop’s <em>Pattern Recognition and Machine Learning</em>, which I recommend taking a look at for additional theoretical intuition.</p>
<p>So let’s get started! <a href="https://www.cs.princeton.edu/~bee/courses/hw/points_hw4.txt">Our data</a> comes from Barbara Englehardt’s Spring 2013 Duke course, <a href="https://www.cs.princeton.edu/~bee/courses/cbb540.html">STA613/CBB540: Statistical methods in computational biology</a>, <a href="https://www.cs.princeton.edu/~bee/courses/hw/sta613cbb540_hw4.pdf">homework 4</a>.</p>
</div>
<div id="load-the-data" class="section level2">
<h2>Load the data</h2>
<p>First, we load our library functions and data points:</p>
<pre class="r"><code>library(deldir)
library(ellipse)
library(pryr)
library(ggplot2)
center_title = theme(plot.title = element_text(hjust = 0.5))
no_legend = theme(legend.position=&quot;none&quot;)</code></pre>
<pre class="r"><code>points = read.table(&#39;../data/points_hw4.txt&#39;, col.names=c(&quot;x&quot;, &quot;y&quot;))
ggplot(points, aes(x = x, y = y)) + geom_point() +
  labs(title = &quot;Scatter plot of data&quot;) +
  center_title</code></pre>
<p><img src="/post/2018-01-30-clustering-with-k-means-and-em_files/figure-html/points-1.png" width="672" /></p>
</div>
<div id="clustering-algorithms" class="section level2">
<h2>Clustering algorithms</h2>
<p>One of the aims of this post is to show how the common EM clustering algorithm reduces to K-means in a particular limit. To do this, we should first put both algorithms into a common form.</p>
<p>If you’ve worked through the algorithms before, you’ll see that both K-means and EM consist of a step where points are assigned to clusters, followed by a step where parameter updates are computed from those assignments. They look like the following:</p>
<ul>
<li>EM E step: compute soft assignments of assigning a probability distribution for each point over the <span class="math">\(K\)</span> clusters</li>
<li>K-means “E step”: compute hard assignments of assigning every data point to its nearest cluster center</li>
<li>EM M step: using the soft assignments, update <span class="math">\(\mathbf{\mu}_i\)</span>, the Gaussian means, <span class="math">\(\mathbf{\Sigma}_i\)</span>, the Gaussian covariance matrices, and <span class="math">\(\mathbf{\pi}\)</span>, the cluster weights</li>
<li>K-means “M step”: using the hard assignments, update <span class="math">\(\mathbf{\mu}_i\)</span>, the cluster centers</li>
</ul>
<p>Our approach will be to implement these four helper functions, and then string them together using a common interface. This also cuts down on duplication. The high-level program takes an E function, an M function, and starting inputs to an E step, and alternates the two steps while keeping track of all intermediate results:</p>
<pre class="r"><code>iterate_em = function(nsteps, K, points, e_step, m_step, m_params.init) {
  m_list = list(m_params.init)
  e_list = list()
  i = 1
  while (i &lt;= nsteps) {
    e_list[[i]] = e_step(K, points, m_list[[i]])
    m_list[[i+1]] = m_step(K, points, e_list[[i]])
    i = i + 1
  }
  return(list(m_list=m_list, e_list=e_list))
}</code></pre>
<p>The rest of this section is pretty dry, and just consists of my R code for the two algorithms. In the E step for EM, I make use of the log-sum-exp trick, which turns out to be helpful for numerical precision; you can read more about that <a href="https://hips.seas.harvard.edu/blog/2013/01/09/computing-log-sum-exp/">here</a>.</p>
<pre class="r"><code># K-means as functions
# points: N x D matrix
# e_params: list with
#   clusters: vector of assignments
# m_params: list with
#   centers: matrix of cluster centers
kmeans.e = function(K, points, m_params) {
  N = dim(points)[1]
  D = dim(points)[2]
  
  distances2 = matrix(0, N, K)
  for (k in 1:K) {
    for (j in 1:D) {
      distances2[,k] = distances2[,k] + (points[,j] - m_params$centers[k,j])^2
    }
  }
  clusters = apply(distances2, 1, which.min)
  e_params = list(clusters=clusters)
  
  return(e_params)
}

kmeans.m = function(K, points, e_params) {
  N = dim(points)[1]
  D = dim(points)[2]
  
  centers = matrix(0, K, D)
  for (k in 1:K) {
    centers[k,] = colMeans(points[e_params$clusters == k,])
  }
  m_params = list(centers=centers)
  
  return(m_params)
}

# EM as functions
# points: N x D matrix
# m_params: list with
#   mu: K x D, MoG centers
#   sigma: list of length K of D x D matrices, MoG covariances
#   weights: K, MoG weights
# e_params: list with
#   resp: responsibilities, N x K
#   ll: log-likelihood, for debugging
em.e = function(K, points, m_params) {
  N = dim(points)[1]
  D = dim(points)[2]
  mu = m_params$mu
  sigma = m_params$sigma
  weights = m_params$weights
  
  # update responsibilities
  resp = matrix(rep(0, N*K), N, K)
  for (k in 1:K) {
    constant_k = log(weights[k]) - 0.5*log(det(sigma[[k]])) -
      log(2*pi)*(D/2)
    displacement = points - as.numeric(matrix(mu[k,], N, D, byrow = TRUE))
    log_probs = -1/2 * colSums(t(displacement) * (
      solve(sigma[[k]]) %*% t(displacement)))
    resp[,k] = log_probs + constant_k
  }
  
  # log-sum-exp trick
  max_log_probs = apply(resp, 1, max)
  resp = resp - matrix(max_log_probs, N, K)
  resp = exp(resp)
  ll = mean(log(rowSums(resp))) + mean(max_log_probs)  # log likelihood
  resp = resp / matrix(rowSums(resp), N, K)
  
  e_params = list(resp=resp, ll=ll)
  return(e_params)
}

em.m = function(K, points, e_params, fix_sigma=NULL, fix_weights=NULL) {
  N = dim(points)[1]
  D = dim(points)[2]
  resp = e_params$resp
  
  # update means
  mu = matrix(0, K, D)
  for (k in 1:K) {
    mu[k,] = colSums(resp[,k]*points) / sum(resp[,k])
  }

  # update covarainces
  if (is.null(fix_sigma)) {
    sigma = NULL
    for (k in 1:K) {
      sigma[[k]] = matrix(0, D, D)
      displacement = points - as.numeric(matrix(mu[k,], N, D, byrow = TRUE))
      for (j in 1:D) {
        sigma[[k]][j,] = colSums(displacement[,j]*displacement*resp[,k]) / sum(resp[,k])
      }
    }
  } else {
    sigma = fix_sigma
  }
  
  # update component weights
  if (is.null(fix_weights)) {
    weights = colSums(resp) / sum(resp)
  } else {
    weights = fix_weights
  }
  
  m_params = list(mu=mu, sigma=sigma, weights=weights)
  return(m_params)
}</code></pre>
</div>
<div id="initial-run" class="section level2">
<h2>Initial run</h2>
<p>Now, we can choose <code>K = 3</code> and <code>nsteps = 20</code> for an initial run. We randomly choose three points as our starting centers for both K-means and EM. For EM, we additionaly initialize using identity covariance and equal weights over the mixture components.</p>
<pre class="r"><code># Run K means and EM
K = 3
nsteps = 20
N = dim(points)[1]
D = dim(points)[2]
set.seed(3)
centers = points[sample(1:N, K),]
row.names(centers) = NULL
m_params.init = list(centers=centers)
kmeans_results = iterate_em(nsteps, K, points, kmeans.e, kmeans.m, m_params.init)

mu = centers
sigma = NULL
for(k in 1:K) {
  sigma[[k]] = diag(D)  # covariances initialized to identity matrix
}
weights = rep(1, K) / K  # weights initialized to uniform
m_params.init = list(mu=mu, sigma=sigma, weights=weights)
em_results = iterate_em(nsteps, K, points, em.e, em.m, m_params.init)</code></pre>
<p>The results of each E-step produces data for each of the N points, which is verbose. Instead, let’s print the results of the M-step, which is more compact.</p>
<pre class="r"><code>kmeans_results$m_list[1:3]</code></pre>
<pre><code>## [[1]]
## [[1]]$centers
##          x        y
## 1 4.236116 4.322595
## 2 3.115771 3.307241
## 3 4.474370 2.387970
## 
## 
## [[2]]
## [[2]]$centers
##          [,1]     [,2]
## [1,] 3.903263 4.585723
## [2,] 2.434461 3.479530
## [3,] 4.907858 2.282699
## 
## 
## [[3]]
## [[3]]$centers
##          [,1]     [,2]
## [1,] 3.837077 4.520047
## [2,] 2.366460 3.438372
## [3,] 4.907858 2.282699</code></pre>
<pre class="r"><code>em_results$m_list[1:3]</code></pre>
<pre><code>## [[1]]
## [[1]]$mu
##          x        y
## 1 4.236116 4.322595
## 2 3.115771 3.307241
## 3 4.474370 2.387970
## 
## [[1]]$sigma
## [[1]]$sigma[[1]]
##      [,1] [,2]
## [1,]    1    0
## [2,]    0    1
## 
## [[1]]$sigma[[2]]
##      [,1] [,2]
## [1,]    1    0
## [2,]    0    1
## 
## [[1]]$sigma[[3]]
##      [,1] [,2]
## [1,]    1    0
## [2,]    0    1
## 
## 
## [[1]]$weights
## [1] 0.3333333 0.3333333 0.3333333
## 
## 
## [[2]]
## [[2]]$mu
##          [,1]     [,2]
## [1,] 3.596083 4.280319
## [2,] 2.652853 3.548945
## [3,] 4.318030 2.668293
## 
## [[2]]$sigma
## [[2]]$sigma[[1]]
##            [,1]       [,2]
## [1,] 0.66681006 0.05709793
## [2,] 0.05709793 0.56318406
## 
## [[2]]$sigma[[2]]
##           [,1]      [,2]
## [1,] 0.6406161 0.1080031
## [2,] 0.1080031 0.5753433
## 
## [[2]]$sigma[[3]]
##            [,1]       [,2]
## [1,]  1.3820866 -0.4615455
## [2,] -0.4615455  0.7275576
## 
## 
## [[2]]$weights
## [1] 0.3032386 0.5042118 0.1925496
## 
## 
## [[3]]
## [[3]]$mu
##          [,1]     [,2]
## [1,] 3.602189 4.367311
## [2,] 2.578687 3.553098
## [3,] 4.475524 2.552581
## 
## [[3]]$sigma
## [[3]]$sigma[[1]]
##           [,1]      [,2]
## [1,] 0.5694985 0.1191194
## [2,] 0.1191194 0.4292038
## 
## [[3]]$sigma[[2]]
##           [,1]      [,2]
## [1,] 0.5033149 0.1763752
## [2,] 0.1763752 0.5461334
## 
## [[3]]$sigma[[3]]
##            [,1]       [,2]
## [1,]  1.2389029 -0.4628892
## [2,] -0.4628892  0.5710045
## 
## 
## [[3]]$weights
## [1] 0.3006975 0.5026309 0.1966716</code></pre>
<p>Looks sensible. It’s also a good idea to check that the EM log-likelihood always increases.</p>
<pre class="r"><code>lls = rep(0, nsteps)
for (i in 1:nsteps) {
  lls[i] = em_results$e_list[[i]]$ll
}
ggplot(data=data.frame(x=1:nsteps, y=lls)) +
  geom_line(aes(x=x, y=y)) + geom_point(aes(x=x, y=y)) +
  labs(title=&quot;Log likelihood for EM&quot;, x=&quot;step&quot;, y=&quot;log likelihood&quot;) +
  center_title</code></pre>
<p><img src="/post/2018-01-30-clustering-with-k-means-and-em_files/figure-html/log-likelihood-1.png" width="672" /></p>
</div>
<div id="visualization-code" class="section level2">
<h2>Visualization code</h2>
<p>We’d like to visualize our results, and since my aim is to compare K-means with EM, I’ve chosen to visualize them side-by-side using <code>ggplot</code>’s <code>facet_grid</code> option. Points are colored to show the assigned cluster (K-means) or the most likely cluster (EM); an alternate visualization would use blended colors for EM. I used the <code>deldir</code> package to compute K-means decision boundaries, which come from Voronoi diagrams, and the <code>ellipse</code> package to plot shapes of each Gaussian mixture.</p>
<pre class="r"><code>make_visualization = function(points, kmeans_data, em_data, nsteps, K) {
  for (i in 1:nsteps) {
    # colored points
    df_points = rbind(
      data.frame(x = points[,1], y = points[,2], type = &quot;K-means&quot;,
                 cluster = kmeans_data$e_list[[i]]$clusters),
      data.frame(x = points[,1], y = points[,2], type = &quot;EM&quot;,
                 cluster = apply(em_data$e_list[[i]]$resp, 1, which.max)))

    # K-means decision boundaries
    centers = kmeans_data$m_list[[i]]$centers
    df_voronoi = deldir(centers[,1], centers[,2])$dirsgs
    df_voronoi$type = factor(&quot;K-means&quot;, levels=c(&quot;K-means&quot;, &quot;EM&quot;))
    
    # ellipses
    mu = em_data$m_list[[i]]$mu
    sigma = em_data$m_list[[i]]$sigma
    all_ellipses = NULL
    for (k in 1:K) {
      ellipse_data = ellipse(sigma[[k]], level=pchisq(1, df=D))
      all_ellipses[[k]] = data.frame(
        x=ellipse_data[,1] + mu[k,1], y=ellipse_data[,2] + mu[k,2],
        cluster=k, type=&quot;EM&quot;)
    }
    df_ellipses = do.call(rbind, all_ellipses)
    
    print(
      ggplot() +
        geom_point(data=df_points, aes(x=x, y=y, color=factor(cluster))) +
        geom_point(data=data.frame(x=centers[,1], y=centers[,2], type=&quot;K-means&quot;),
                   aes(x=x, y=y), shape=17, size=3) +
        geom_segment(data=df_voronoi, linetype = 1, color= &quot;#FFB958&quot;,
                     aes(x = x1, y = y1, xend = x2, yend = y2)) +
        geom_path(data=df_ellipses, aes(x=x, y=y, color=factor(cluster))) +
        facet_grid(. ~ type) +
        ggtitle(paste0(&quot;Most likely cluster, K = &quot;, K, &quot;, step = &quot;, i)) +
        center_title + no_legend)
  }
}</code></pre>
<p>Since <code>knitr</code> / R Markdown <a href="https://grunwaldlab.github.io/Reproducible-science-in-R/Extra_content---Advanced_RMarkdown.html">supports animations</a>, we can simply plot each frame in a for loop. In my case, I’m using <code>ffmpeg</code> with an <code>.mp4</code> format, and hacked <code>knitr</code> to add <a href="https://apple.stackexchange.com/questions/166553/why-wont-video-from-ffmpeg-show-in-quicktime-imovie-or-quick-preview">some flags</a> for Apple support, which was necessary for me to get things (hopefully) viewable in Safari.</p>
<p>With this visualization code, we can finally take a look at our results!</p>
<pre class="r"><code>make_visualization(points, kmeans_results, em_results, nsteps, K)</code></pre>
<video width="672"  controls loop>
<source src="/post/2018-01-30-clustering-with-k-means-and-em_files/figure-html/k3-base.mp4" />
</video>

</div>
<div id="the-k-means-limit" class="section level2">
<h2>The K-means limit</h2>
<p>To make the EM algorithm more like K-means, we start by limiting the M step to only change the <span class="math">\(\mathbf{\mu}\)</span> parameters. The correspondence is pretty clear – the Gaussian means correspond to the K-means cluster centers.</p>
<p>If you look closely above, I added some extra arguments to the EM M step that allows for this change. Since the <code>iterate_em</code> function accepts a function for the M step, we can use the <code>partial</code> function from the <code>pryr</code> package to set these arguments appropriately.</p>
<pre class="r"><code>fixed_sigma = partial(em.m, fix_sigma=sigma, fix_weights=weights)
em_results = iterate_em(nsteps, K, points, em.e, fixed_sigma, m_params.init)
make_visualization(points, kmeans_results, em_results, nsteps, K)</code></pre>
<video width="672"  controls loop>
<source src="/post/2018-01-30-clustering-with-k-means-and-em_files/figure-html/k3-fixed-big.mp4" />
</video>

<p>In the above animation, you’ll see that the shapes of the Gaussians do not change; only their centers do. One can show that this leads to linear decision boundaries for the most likely cluster, just like K-means. However, the algorithm evolution is still not the same as K-means.</p>
<p>To allow the two algorithms to finally match up, we need to take a limit where the fixed covariance for each mixture component is <span class="math">\(\epsilon I\)</span>, and we take <span class="math">\(\epsilon\)</span> to 0. In this case, we take <span class="math">\(\epsilon\)</span> to be 0.01, corresponding to a standard deviation of 0.1. The log-sum-exp trick I mentioned earlier was necessary for my results to not under / overflow in this case.</p>
<pre class="r"><code>sigma001 = NULL
for(k in 1:K) {
  sigma001[[k]] = diag(D)*0.01
}
m_params.init = list(mu=mu, sigma=sigma001, weights=weights)
fixed_sigma001 = partial(em.m, fix_sigma=sigma001, fix_weights=weights)
em_results = iterate_em(nsteps, K, points, em.e, fixed_sigma001, m_params.init)
make_visualization(points, kmeans_results, em_results, nsteps, K)</code></pre>
<video width="672"  controls loop>
<source src="/post/2018-01-30-clustering-with-k-means-and-em_files/figure-html/k3-fixed-small.mp4" />
</video>

<p>The two sides match very well!</p>
</div>
<div id="extra-k-8" class="section level2">
<h2>Extra: K = 8</h2>
<p>We can repeat this entire process for a different value of <span class="math">\(K\)</span>. With regular EM:</p>
<pre class="r"><code># Run K means and EM
K = 8
set.seed(3)
centers = points[sample(1:N, K),]
row.names(centers) = NULL
m_params.init = list(centers=centers)
kmeans_results = iterate_em(nsteps, K, points, kmeans.e, kmeans.m, m_params.init)

mu = centers
sigma = NULL
for(k in 1:K) {
  sigma[[k]] = diag(D)  # covariances initialized to identity matrix
}
weights = rep(1, K) / K  # weights initialized to uniform
m_params.init = list(mu=mu, sigma=sigma, weights=weights)
em_results = iterate_em(nsteps, K, points, em.e, em.m, m_params.init)

# Visualize
make_visualization(points, kmeans_results, em_results, nsteps, K)</code></pre>
<video width="672"  controls loop>
<source src="/post/2018-01-30-clustering-with-k-means-and-em_files/figure-html/k8-base.mp4" />
</video>

<p>With only <span class="math">\(\mathbf{\mu}\)</span> updates, and <span class="math">\(I\)</span> covariance:</p>
<pre class="r"><code>fixed_sigma = partial(em.m, fix_sigma=sigma, fix_weights=weights)
em_results = iterate_em(nsteps, K, points, em.e, fixed_sigma, m_params.init)
make_visualization(points, kmeans_results, em_results, nsteps, K)</code></pre>
<video width="672"  controls loop>
<source src="/post/2018-01-30-clustering-with-k-means-and-em_files/figure-html/k8-fixed-big.mp4" />
</video>

<p>With only <span class="math">\(\mathbf{\mu}\)</span> updates, and <span class="math">\(0.01I\)</span> covariance:</p>
<pre class="r"><code>sigma001 = NULL
for(k in 1:K) {
  sigma001[[k]] = diag(D)*0.01
}
m_params.init = list(mu=mu, sigma=sigma001, weights=weights)
fixed_sigma001 = partial(em.m, fix_sigma=sigma001, fix_weights=weights)
em_results = iterate_em(nsteps, K, points, em.e, fixed_sigma001, m_params.init)
make_visualization(points, kmeans_results, em_results, nsteps, K)</code></pre>
<video width="672"  controls loop>
<source src="/post/2018-01-30-clustering-with-k-means-and-em_files/figure-html/k8-fixed-small.mp4" />
</video>

</div>
<div id="references" class="section level2">
<h2>References</h2>
<p>Besides the links included above, I found <a href="http://letstalkdata.com/2014/05/creating-voronoi-diagrams-with-ggplot/">this link</a> useful for plotting Voronoi diagrams using <code>ggplot</code>.</p>
<p><strong><em>This blog post was generated from an R Markdown file using the <code>knitr</code> and <code>blogdown</code> packages. The original source can be downloaded <a href="https://github.com/brianzhang01/brianzhang01.github.io/blob/master/post/2018-01-30-clustering-with-k-means-and-em.Rmd">from GitHub</a>.</em></strong></p>
</div>

		</div><div id="disqus_thread"></div>
<script type="text/javascript">
	(function () {
		
		
		if (window.location.hostname == "localhost")
			return;

		var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
		var disqus_shortname = 'brianzhang01-stats';
		dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
		(document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
	})();
</script>
<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by
		Disqus.</a></noscript>
<a href="http://disqus.com/" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
</div>
	<div class="footer wrapper">
	<nav class="nav">
		<div> Built with <a href="https://gohugo.io">Hugo</a>, <a href="https://github.com/rstudio/blogdown">blogdown</a>, and <a href="https://github.com/vividvilla/ezhil">Ezhil theme</a></div>
	</nav>
</div>

<script src="//yihui.name/js/math-code.js"></script>
<script async
src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'UA-113303717-1', 'auto');
	
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>
<script>feather.replace()</script>
</body>
</html>
